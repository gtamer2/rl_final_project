from trl import PPOConfig
from transformers import AutoTokenizer
from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, AutoModelForSeq2SeqLMWithValueHead 
from transformers import pipeline
from tqdm import tqdm
# import torch
from utils.dataset import getDataset, load_tokenizer
from utils.reward import getScores
import torch
import gc
# import copy

BATCH_SIZE = 32

def load_ppo_config():
    # The PPOConfig dataclass controls all the hyperparameters and settings for the PPO algorithm and trainer.
    # All parameters and their default values can be found here: https://huggingface.co/docs/trl/v0.8.6/en/trainer#trl.PPOConfig
    return PPOConfig(
        task_name="ppo-for-rlhf-on-gpt-neo",
        model_name="google-t5/t5-small",
        learning_rate=1.41e-5,
        seed=0,
        batch_size=BATCH_SIZE,
        mini_batch_size=BATCH_SIZE//2
    )


def load_model(model_name: str):
    # See: https://huggingface.co/EleutherAI/gpt-neo-125m
    # Initialize our model. Note that PPO also requires a reference model, but this model is generated by the â€˜PPOTrainer` automatically.
    # generator = pipeline('text-generation', model='EleutherAI/gpt-neo-125M')
    # return AutoModelForCausalLMWithValueHead.from_pretrained(model_name)
    return AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_name)


# def load_tokenizer(model_name: str):
#     # HF automatically pulls the correct tokenizer for the model
#     tokenizer = AutoTokenizer.from_pretrained(model_name)
#     tokenizer.pad_token = tokenizer.eos_token
#     return tokenizer


def build_ppo_trainer(model, config, dataset, tokenizer):
    return PPOTrainer(
        model=model,
        config=config,
        dataset=dataset,
        tokenizer=tokenizer,
    )


def perform_rlhf_ppo_training():
    # Get PPO trainer
    config = load_ppo_config()
    language_model = load_model(model_name=config.model_name)
    # ref_language_model = copy.deepcopy(language_model)
    tokenizer = load_tokenizer(model_name=config.model_name)
    dataset = getDataset(dataset_size=68, batch_size=BATCH_SIZE)
    ppo_trainer = build_ppo_trainer(language_model, config, dataset, tokenizer) 
    
    responseDict = {}
    for i in range(len(dataset)):
        responseDict[str(dataset[i]["query"])] = dataset[i]["chosen"]

    # Train the model
    
    generation_kwargs = {
        "min_length": 30,
        "top_k": 10,
        "top_p": 1.0,
        "do_sample": True,
        "pad_token_id": tokenizer.eos_token_id,
        "max_new_tokens": 128
    }

    epochs = 10
    for epoch in range(epochs):
        print(f"Epoch {epoch}/{epochs}")
        for batch in tqdm(ppo_trainer.dataloader):
            # Clear CUDA cache
            torch.cuda.empty_cache()

            # Perform garbage collection
            gc.collect()
            
            query_tensors = batch["input_ids"]
            query_tensors = torch.stack(query_tensors)            
            query_tensors = torch.transpose(query_tensors, 0, 1)
            query_tensors = torch.split(query_tensors, 1, dim=0)
            query_tensors = [t.squeeze(0) for t in query_tensors]
        
            #### Get response from SFTModel
            response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)
            
            batch["response"] = [[tokenizer.decode(r.squeeze())] for r in response_tensors]
            
            batch["chosen"] = []
            
            c = 0
            for k in batch["query"]:
                try:
                    batch["chosen"].append(responseDict[k])
                    c += 1
                except:
                    batch["chosen"].append([{},{"content":""}])
        
            #### Compute reward score
            # our code
            rewards = getScores(
                inputs= batch["query"],
                candidates= batch["response"],
                ref_candidates=batch["chosen"],
                batch_size=BATCH_SIZE
            )
            
            rewards = [torch.tensor(k) for k in rewards]
        
            #### Run PPO step
            stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
            ppo_trainer.log_stats(stats, batch, rewards)
            
            del batch
            del query_tensors
            del response_tensors
            del stats
            del rewards

    #### Save model
    ppo_trainer.save_pretrained("my_ppo_model")