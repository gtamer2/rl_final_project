from trl import PPOConfig
from transformers import AutoTokenizer
from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer
from transformers import pipeline
from tqdm import tqdm
import torch


def load_ppo_config():
    # The PPOConfig dataclass controls all the hyperparameters and settings for the PPO algorithm and trainer.
    # All parameters and their default values can be found here: https://huggingface.co/docs/trl/v0.8.6/en/trainer#trl.PPOConfig
    return PPOConfig(
        task_name="ppo-for-rlhf-on-gpt-neo",
        model_name="gpt-neo",
        learning_rate=1.41e-5,
        seed=0,
    )


def load_model(model_name: str):
    # TODO: replace with our language model
    # Initialize our model. Note that PPO also requires a reference model, but this model is generated by the â€˜PPOTrainer` automatically. The model can be initialized as follows:
    generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')

    return AutoModelForCausalLMWithValueHead.from_pretrained(model_name)


def load_tokenizer(model_name: str):
    # TODO: replace with euler tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token


def load_dataset():
    # TODO
    return pipeline("text-generation", model="lvwerra/distilbert-imdb")


def build_ppo_trainer(model, config, dataset, tokenizer):
    return PPOTrainer(
        model=model,
        config=config,
        dataset=dataset,
        tokenizer=tokenizer,
    )


def load_reward_model():
    # TODO: replace with our reward model
    return pipeline("text-classification", model="lvwerra/distilbert-imdb")


def train():
    # Get reward model
    reward_model = load_reward_model()

    # Get PPO trainer
    config = load_ppo_config()
    language_model = load_model(model_name=config.model_name)
    tokenizer = load_tokenizer(model_name=config.model_name)
    dataset = load_dataset()
    ppo_trainer = build_ppo_trainer(language_model, config, dataset, tokenizer)

    # Train the model
    
    generation_kwargs = {
        "min_length": -1,
        "top_k": 0.0,
        "top_p": 1.0,
        "do_sample": True,
        "pad_token_id": tokenizer.eos_token_id,
    }

    epochs = 10
    for epoch in tqdm(range(epochs), "epoch: "):
        print(f"Epoch {epoch}/{epochs}")
        for batch in tqdm(ppo_trainer.dataloader): 
            query_tensors = batch["input_ids"]
        
            #### Get response from SFTModel
            response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)
            batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]
        
            #### Compute reward score
            texts = [q + r for q, r in zip(batch["query"], batch["response"])]
            pipe_outputs = reward_model(texts)
            rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
        
            #### Run PPO step
            stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
            ppo_trainer.log_stats(stats, batch, rewards)

    #### Save model
    ppo_trainer.save_pretrained("my_ppo_model")